//! Performance metrics collection and telemetry for nvbind
//!
//! This module provides comprehensive GPU performance monitoring and metrics collection
//! to validate sub-microsecond GPU passthrough claims and monitor container performance.

use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::RwLock;
use tracing::{debug, info};

/// Real-time performance metrics collector
pub struct MetricsCollector {
    /// Historical metrics storage
    metrics_store: Arc<RwLock<MetricsStore>>,
    /// Active performance sessions
    active_sessions: Arc<Mutex<HashMap<String, PerformanceSession>>>,
    /// Collection configuration
    #[allow(dead_code)]
    config: MetricsConfig,
}

/// Configuration for metrics collection
#[derive(Debug, Clone)]
pub struct MetricsConfig {
    /// Maximum number of metrics to store in memory
    pub max_metrics_history: usize,
    /// Metrics collection interval
    pub collection_interval: Duration,
    /// Enable sub-microsecond latency tracking
    pub enable_latency_tracking: bool,
    /// Enable GPU utilization monitoring
    pub enable_gpu_monitoring: bool,
    /// Export metrics to file
    pub export_enabled: bool,
    /// Export file path
    pub export_path: Option<String>,
}

/// Metrics storage with circular buffer for efficiency
#[derive(Debug)]
struct MetricsStore {
    /// GPU passthrough latency metrics
    gpu_latency: VecDeque<LatencyMetric>,
    /// Container startup performance
    startup_metrics: VecDeque<StartupMetric>,
    /// GPU utilization metrics
    gpu_utilization: VecDeque<UtilizationMetric>,
    /// Memory usage metrics
    memory_metrics: VecDeque<MemoryMetric>,
    /// Thermal metrics
    thermal_metrics: VecDeque<ThermalMetric>,
    /// Custom performance counters
    custom_counters: HashMap<String, VecDeque<CustomMetric>>,
}

/// GPU passthrough latency measurement
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LatencyMetric {
    /// Timestamp
    pub timestamp: u64,
    /// Container ID
    pub container_id: String,
    /// Runtime type (bolt, docker, podman)
    pub runtime: String,
    /// GPU discovery latency (nanoseconds)
    pub gpu_discovery_ns: u64,
    /// CDI generation latency (nanoseconds)
    pub cdi_generation_ns: u64,
    /// Container creation latency (nanoseconds)
    pub container_creation_ns: u64,
    /// GPU device attachment latency (nanoseconds)
    pub gpu_attachment_ns: u64,
    /// Total GPU passthrough latency (nanoseconds)
    pub total_latency_ns: u64,
    /// Success/failure status
    pub success: bool,
    /// Error message if failed
    pub error_message: Option<String>,
}

/// Container startup performance metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StartupMetric {
    /// Timestamp
    pub timestamp: u64,
    /// Container ID
    pub container_id: String,
    /// Runtime type
    pub runtime: String,
    /// Image name
    pub image: String,
    /// Total startup time (milliseconds)
    pub startup_time_ms: u64,
    /// GPU initialization time (milliseconds)
    pub gpu_init_time_ms: u64,
    /// Memory allocated (bytes)
    pub memory_allocated: u64,
    /// GPU memory allocated (bytes)
    pub gpu_memory_allocated: u64,
    /// Success status
    pub success: bool,
}

/// GPU utilization metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UtilizationMetric {
    /// Timestamp
    pub timestamp: u64,
    /// GPU device ID
    pub gpu_id: String,
    /// Container ID using the GPU
    pub container_id: Option<String>,
    /// GPU utilization percentage (0-100)
    pub gpu_utilization: f32,
    /// Memory utilization percentage (0-100)
    pub memory_utilization: f32,
    /// Encoder utilization percentage (0-100)
    pub encoder_utilization: f32,
    /// Decoder utilization percentage (0-100)
    pub decoder_utilization: f32,
    /// Power draw (watts)
    pub power_draw: f32,
    /// Clock speeds
    pub graphics_clock: u32,
    pub memory_clock: u32,
}

/// Memory usage metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryMetric {
    /// Timestamp
    pub timestamp: u64,
    /// Container ID
    pub container_id: String,
    /// Total system memory (bytes)
    pub total_system_memory: u64,
    /// Used system memory (bytes)
    pub used_system_memory: u64,
    /// Total GPU memory (bytes)
    pub total_gpu_memory: u64,
    /// Used GPU memory (bytes)
    pub used_gpu_memory: u64,
    /// nvbind overhead (bytes)
    pub nvbind_overhead: u64,
    /// Container memory usage (bytes)
    pub container_memory: u64,
}

/// Thermal monitoring metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThermalMetric {
    /// Timestamp
    pub timestamp: u64,
    /// GPU device ID
    pub gpu_id: String,
    /// GPU temperature (Celsius)
    pub gpu_temp: i32,
    /// Memory temperature (Celsius)
    pub memory_temp: Option<i32>,
    /// Fan speed percentage
    pub fan_speed: u8,
    /// Thermal throttling active
    pub throttling: bool,
    /// Power limit hit
    pub power_limit: bool,
}

/// Custom performance counter
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CustomMetric {
    /// Timestamp
    pub timestamp: u64,
    /// Metric name
    pub name: String,
    /// Metric value
    pub value: f64,
    /// Associated tags
    pub tags: HashMap<String, String>,
}

/// Performance measurement session
#[derive(Debug)]
pub struct PerformanceSession {
    /// Session ID
    pub session_id: String,
    /// Start time
    pub start_time: Instant,
    /// Session metrics
    pub metrics: Vec<SessionMetric>,
    /// Session tags
    pub tags: HashMap<String, String>,
}

/// Session-specific metric
#[derive(Debug, Clone)]
pub struct SessionMetric {
    /// Metric timestamp
    pub timestamp: Instant,
    /// Metric type
    pub metric_type: SessionMetricType,
    /// Metric value
    pub value: f64,
    /// Additional data
    pub data: HashMap<String, String>,
}

/// Types of session metrics
#[derive(Debug, Clone)]
pub enum SessionMetricType {
    GpuDiscovery,
    DriverDetection,
    CdiGeneration,
    ContainerCreation,
    GpuAttachment,
    ContainerStart,
    Custom(String),
}

/// Performance benchmark results
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResults {
    /// Test name
    pub test_name: String,
    /// Total test duration
    pub total_duration_ns: u64,
    /// Number of iterations
    pub iterations: u32,
    /// Average latency (nanoseconds)
    pub avg_latency_ns: f64,
    /// Minimum latency (nanoseconds)
    pub min_latency_ns: u64,
    /// Maximum latency (nanoseconds)
    pub max_latency_ns: u64,
    /// Standard deviation
    pub std_deviation_ns: f64,
    /// 95th percentile latency
    pub p95_latency_ns: u64,
    /// 99th percentile latency
    pub p99_latency_ns: u64,
    /// Success rate percentage
    pub success_rate: f32,
    /// Additional metadata
    pub metadata: HashMap<String, String>,
}

impl MetricsCollector {
    /// Create new metrics collector
    pub fn new(config: MetricsConfig) -> Self {
        Self {
            metrics_store: Arc::new(RwLock::new(MetricsStore::new(config.max_metrics_history))),
            active_sessions: Arc::new(Mutex::new(HashMap::new())),
            config,
        }
    }

    /// Start performance measurement session
    pub fn start_session(&self, session_id: String, tags: HashMap<String, String>) -> Result<()> {
        let session = PerformanceSession {
            session_id: session_id.clone(),
            start_time: Instant::now(),
            metrics: Vec::new(),
            tags,
        };

        let mut sessions = self.active_sessions.lock().unwrap();
        sessions.insert(session_id.clone(), session);

        debug!("Started performance session: {}", session_id);
        Ok(())
    }

    /// Record latency measurement
    pub async fn record_latency(
        &self,
        session_id: &str,
        metric_type: SessionMetricType,
        latency_ns: u64,
    ) -> Result<()> {
        // Record in active session
        {
            let mut sessions = self.active_sessions.lock().unwrap();
            if let Some(session) = sessions.get_mut(session_id) {
                session.metrics.push(SessionMetric {
                    timestamp: Instant::now(),
                    metric_type: metric_type.clone(),
                    value: latency_ns as f64,
                    data: HashMap::new(),
                });
            }
        }

        // Record in metrics store if it's a GPU latency metric
        if matches!(
            metric_type,
            SessionMetricType::GpuDiscovery | SessionMetricType::GpuAttachment
        ) {
            // Get runtime from session tags
            let runtime = {
                let sessions = self.active_sessions.lock().unwrap();
                sessions
                    .get(session_id)
                    .and_then(|s| s.tags.get("runtime"))
                    .cloned()
                    .unwrap_or_else(|| "unknown".to_string())
            };

            let mut store = self.metrics_store.write().await;
            let metric = LatencyMetric {
                timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs(),
                container_id: session_id.to_string(),
                runtime,
                gpu_discovery_ns: latency_ns,
                cdi_generation_ns: 0,
                container_creation_ns: 0,
                gpu_attachment_ns: 0,
                total_latency_ns: latency_ns,
                success: true,
                error_message: None,
            };

            store.add_latency_metric(metric);
        }

        Ok(())
    }

    /// Measure GPU discovery performance
    pub async fn measure_gpu_discovery<F, R>(&self, operation: F) -> Result<(R, u64)>
    where
        F: FnOnce() -> Result<R>,
    {
        let start = Instant::now();
        let result = operation()?;
        let duration_ns = start.elapsed().as_nanos() as u64;

        debug!("GPU discovery took {} nanoseconds", duration_ns);

        Ok((result, duration_ns))
    }

    /// Measure container creation performance
    pub async fn measure_container_creation<F, R>(
        &self,
        container_id: &str,
        runtime: &str,
        operation: F,
    ) -> Result<(R, StartupMetric)>
    where
        F: FnOnce() -> Result<R>,
    {
        let start = Instant::now();
        let result = operation()?;
        let duration = start.elapsed();

        // Try to get image name from active session
        let image = {
            let sessions = self.active_sessions.lock().unwrap();
            sessions
                .get(container_id)
                .and_then(|s| s.tags.get("image"))
                .cloned()
                .unwrap_or_else(|| "unknown".to_string())
        };

        let metric = StartupMetric {
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs(),
            container_id: container_id.to_string(),
            runtime: runtime.to_string(),
            image,
            startup_time_ms: duration.as_millis() as u64,
            gpu_init_time_ms: 0, // Note: GPU init measured separately in start_session
            memory_allocated: 0, // Note: Memory measured via GPU utilization collection
            gpu_memory_allocated: 0,
            success: true,
        };

        let mut store = self.metrics_store.write().await;
        store.add_startup_metric(metric.clone());

        info!("Container creation took {} ms", duration.as_millis());

        Ok((result, metric))
    }

    /// Collect GPU utilization metrics
    pub async fn collect_gpu_utilization(&self) -> Result<Vec<UtilizationMetric>> {
        let gpus = crate::gpu::discover_gpus().await?;
        let mut metrics = Vec::new();

        for gpu in gpus {
            let utilization = self.get_gpu_utilization(&gpu.id).await?;
            metrics.push(utilization);
        }

        // Store in metrics store
        let mut store = self.metrics_store.write().await;
        for metric in &metrics {
            store.add_utilization_metric(metric.clone());
        }

        Ok(metrics)
    }

    /// Get GPU utilization for specific device
    async fn get_gpu_utilization(&self, gpu_id: &str) -> Result<UtilizationMetric> {
        // Use nvidia-smi to get utilization data
        let output = std::process::Command::new("nvidia-smi")
            .args([
                "--query-gpu=utilization.gpu,utilization.memory,utilization.encoder,utilization.decoder,power.draw,clocks.gr,clocks.mem",
                "--format=csv,noheader,nounits",
                "--id", gpu_id
            ])
            .output()
            .context("Failed to query GPU utilization")?;

        let output_str = String::from_utf8_lossy(&output.stdout);
        let parts: Vec<&str> = output_str.trim().split(',').collect();

        if parts.len() >= 7 {
            // Try to find a container associated with this GPU from active sessions
            let container_id = {
                let sessions = self.active_sessions.lock().unwrap();
                sessions
                    .values()
                    .find(|s| s.tags.get("gpu_id").map(|id| id == gpu_id).unwrap_or(false))
                    .map(|s| s.session_id.clone())
            };

            Ok(UtilizationMetric {
                timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs(),
                gpu_id: gpu_id.to_string(),
                container_id,
                gpu_utilization: parts[0].trim().parse().unwrap_or(0.0),
                memory_utilization: parts[1].trim().parse().unwrap_or(0.0),
                encoder_utilization: parts[2].trim().parse().unwrap_or(0.0),
                decoder_utilization: parts[3].trim().parse().unwrap_or(0.0),
                power_draw: parts[4].trim().parse().unwrap_or(0.0),
                graphics_clock: parts[5].trim().parse().unwrap_or(0),
                memory_clock: parts[6].trim().parse().unwrap_or(0),
            })
        } else {
            Err(anyhow::anyhow!("Invalid nvidia-smi output format"))
        }
    }

    /// End performance session and get results
    pub fn end_session(&self, session_id: &str) -> Result<BenchmarkResults> {
        let mut sessions = self.active_sessions.lock().unwrap();
        let session = sessions
            .remove(session_id)
            .ok_or_else(|| anyhow::anyhow!("Session not found: {}", session_id))?;

        let total_duration = session.start_time.elapsed();
        let latencies: Vec<f64> = session.metrics.iter().map(|m| m.value).collect();

        if latencies.is_empty() {
            return Ok(BenchmarkResults {
                test_name: session_id.to_string(),
                total_duration_ns: total_duration.as_nanos() as u64,
                iterations: 0,
                avg_latency_ns: 0.0,
                min_latency_ns: 0,
                max_latency_ns: 0,
                std_deviation_ns: 0.0,
                p95_latency_ns: 0,
                p99_latency_ns: 0,
                success_rate: 0.0,
                metadata: session.tags,
            });
        }

        let avg = latencies.iter().sum::<f64>() / latencies.len() as f64;
        let min = latencies.iter().fold(f64::INFINITY, |a, &b| a.min(b)) as u64;
        let max = latencies.iter().fold(0.0f64, |a, &b| a.max(b)) as u64;

        // Calculate standard deviation
        let variance =
            latencies.iter().map(|x| (x - avg).powi(2)).sum::<f64>() / latencies.len() as f64;
        let std_dev = variance.sqrt();

        // Calculate percentiles
        let mut sorted_latencies = latencies.clone();
        sorted_latencies.sort_by(|a, b| a.partial_cmp(b).unwrap());

        let p95_index = (sorted_latencies.len() as f64 * 0.95) as usize;
        let p99_index = (sorted_latencies.len() as f64 * 0.99) as usize;

        let p95 = sorted_latencies.get(p95_index).copied().unwrap_or(0.0) as u64;
        let p99 = sorted_latencies.get(p99_index).copied().unwrap_or(0.0) as u64;

        // Calculate success rate from session metrics
        // Count metrics with value > 0 as successful operations
        let successful_ops = session.metrics.iter().filter(|m| m.value > 0.0).count();
        let total_ops = session.metrics.len().max(1); // Avoid division by zero
        let success_rate = ((successful_ops as f64 / total_ops as f64) * 100.0) as f32;

        let results = BenchmarkResults {
            test_name: session_id.to_string(),
            total_duration_ns: total_duration.as_nanos() as u64,
            iterations: latencies.len() as u32,
            avg_latency_ns: avg,
            min_latency_ns: min,
            max_latency_ns: max,
            std_deviation_ns: std_dev,
            p95_latency_ns: p95,
            p99_latency_ns: p99,
            success_rate,
            metadata: session.tags,
        };

        info!(
            "Session {} completed: avg={}ns, p95={}ns, p99={}ns",
            session_id, avg as u64, p95, p99
        );

        Ok(results)
    }

    /// Export metrics to file
    pub async fn export_metrics(&self, file_path: &str) -> Result<()> {
        let store = self.metrics_store.read().await;
        let export_data = serde_json::json!({
            "latency_metrics": store.gpu_latency,
            "startup_metrics": store.startup_metrics,
            "utilization_metrics": store.gpu_utilization,
            "memory_metrics": store.memory_metrics,
            "thermal_metrics": store.thermal_metrics,
            "custom_counters": store.custom_counters,
        });

        tokio::fs::write(file_path, serde_json::to_string_pretty(&export_data)?)
            .await
            .context("Failed to write metrics file")?;

        info!("Metrics exported to: {}", file_path);
        Ok(())
    }

    /// Get real-time performance summary
    pub async fn get_performance_summary(&self) -> Result<PerformanceSummary> {
        let store = self.metrics_store.read().await;

        let avg_latency = if !store.gpu_latency.is_empty() {
            store
                .gpu_latency
                .iter()
                .map(|m| m.total_latency_ns as f64)
                .sum::<f64>()
                / store.gpu_latency.len() as f64
        } else {
            0.0
        };

        let avg_startup_time = if !store.startup_metrics.is_empty() {
            store
                .startup_metrics
                .iter()
                .map(|m| m.startup_time_ms as f64)
                .sum::<f64>()
                / store.startup_metrics.len() as f64
        } else {
            0.0
        };

        Ok(PerformanceSummary {
            average_gpu_latency_ns: avg_latency as u64,
            average_startup_time_ms: avg_startup_time as u64,
            total_containers_created: store.startup_metrics.len(),
            sub_microsecond_achieved: avg_latency < 1000.0, // < 1Î¼s
            metrics_collected: store.gpu_latency.len() + store.startup_metrics.len(),
        })
    }
}

/// Performance summary statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceSummary {
    /// Average GPU passthrough latency
    pub average_gpu_latency_ns: u64,
    /// Average container startup time
    pub average_startup_time_ms: u64,
    /// Total containers created
    pub total_containers_created: usize,
    /// Whether sub-microsecond latency was achieved
    pub sub_microsecond_achieved: bool,
    /// Total metrics collected
    pub metrics_collected: usize,
}

impl MetricsStore {
    fn new(max_size: usize) -> Self {
        Self {
            gpu_latency: VecDeque::with_capacity(max_size),
            startup_metrics: VecDeque::with_capacity(max_size),
            gpu_utilization: VecDeque::with_capacity(max_size),
            memory_metrics: VecDeque::with_capacity(max_size),
            thermal_metrics: VecDeque::with_capacity(max_size),
            custom_counters: HashMap::new(),
        }
    }

    fn add_latency_metric(&mut self, metric: LatencyMetric) {
        if self.gpu_latency.len() >= self.gpu_latency.capacity() {
            self.gpu_latency.pop_front();
        }
        self.gpu_latency.push_back(metric);
    }

    fn add_startup_metric(&mut self, metric: StartupMetric) {
        if self.startup_metrics.len() >= self.startup_metrics.capacity() {
            self.startup_metrics.pop_front();
        }
        self.startup_metrics.push_back(metric);
    }

    fn add_utilization_metric(&mut self, metric: UtilizationMetric) {
        if self.gpu_utilization.len() >= self.gpu_utilization.capacity() {
            self.gpu_utilization.pop_front();
        }
        self.gpu_utilization.push_back(metric);
    }
}

impl Default for MetricsConfig {
    fn default() -> Self {
        Self {
            max_metrics_history: 10000,
            collection_interval: Duration::from_secs(1),
            enable_latency_tracking: true,
            enable_gpu_monitoring: true,
            export_enabled: false,
            export_path: None,
        }
    }
}

/// Create metrics collector with default configuration
pub fn create_default_metrics_collector() -> MetricsCollector {
    MetricsCollector::new(MetricsConfig::default())
}

/// Create metrics collector optimized for Bolt integration
pub fn create_bolt_metrics_collector() -> MetricsCollector {
    let config = MetricsConfig {
        max_metrics_history: 50000, // Larger history for Bolt capsules
        collection_interval: Duration::from_millis(100), // Higher frequency
        enable_latency_tracking: true,
        enable_gpu_monitoring: true,
        export_enabled: true,
        export_path: Some("/var/log/nvbind/bolt-metrics.json".to_string()),
    };

    MetricsCollector::new(config)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_metrics_collector_creation() {
        let collector = create_default_metrics_collector();

        // Start a test session
        let mut tags = HashMap::new();
        tags.insert("runtime".to_string(), "bolt".to_string());

        collector
            .start_session("test-session".to_string(), tags)
            .unwrap();

        // Record some latency measurements
        collector
            .record_latency("test-session", SessionMetricType::GpuDiscovery, 50000)
            .await
            .unwrap();
        collector
            .record_latency("test-session", SessionMetricType::GpuAttachment, 75000)
            .await
            .unwrap();

        // End session and get results
        let results = collector.end_session("test-session").unwrap();

        assert_eq!(results.iterations, 2);
        assert!(results.avg_latency_ns > 0.0);
    }

    #[test]
    fn test_benchmark_results_serialization() {
        let mut metadata = HashMap::new();
        metadata.insert("runtime".to_string(), "bolt".to_string());

        let results = BenchmarkResults {
            test_name: "gpu_passthrough_test".to_string(),
            total_duration_ns: 1000000,
            iterations: 100,
            avg_latency_ns: 50000.0,
            min_latency_ns: 25000,
            max_latency_ns: 100000,
            std_deviation_ns: 15000.0,
            p95_latency_ns: 80000,
            p99_latency_ns: 95000,
            success_rate: 99.0,
            metadata,
        };

        let json = serde_json::to_string(&results).unwrap();
        let deserialized: BenchmarkResults = serde_json::from_str(&json).unwrap();

        assert_eq!(results.test_name, deserialized.test_name);
        assert_eq!(results.avg_latency_ns, deserialized.avg_latency_ns);
    }
}
